# Model-Based RL

## Why Learn the Model?

> [!info]
> 
> 我们这里的 model 专指的是 dynamics model，不是 reward model（当然，reward model 也可以学。都可以学）

如果我们知道了 model（同时我们知道奖励模型），那么如果

- $f(s_t,a_t) = s_{t+1}$，也就是 deterministic dynamics
- $p(s_{t+1} | s_t, a_t)$，也就是 stochastic dynamics
	- 但是，如果要与 iLQR 适配的话，必须有 $s_{t+1} \sim \mathcal N(\mu(s_t, a_t), \Sigma(s_t, a_t))$，也就是符合 normal distribution<br>当然，方差矩阵 $\Sigma$ 在这里与我们的计算无关，因此**根本不需要拟合它**，我们只需要（存疑）

我们可以直接用 iLQR 的方法进行 planning，从而通过 iterative 的方式，很快求出最优路径。

## Model-Based RL Version 0.5

1. run base policy $\pi_0\left(\mathbf{a}_t \mid \mathbf{s}_t\right)$ (e.g., random policy) to collect $\mathcal{D}=\left\{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)_i\right\}$
2. learn dynamics model $f(\mathbf{s}, \mathbf{a})$ to minimize $\sum_i\left\|f\left(\mathbf{s}_i, \mathbf{a}_i\right)-\mathbf{s}_i^{\prime}\right\|^2$
3. plan through $f(\mathbf{s}, \mathbf{a})$ to choose actions

> [!warning] Problem
> 
> $p_{\pi_f}(s_t) \neq p_{\pi_0}(s_t)$，也就是，使用 base policy $\pi_0$ 进行采样的 state 分布，和用拟合后的 $f$ 进行 planning 所得的 $\pi_f$ 的分布，差距很大。很可能造成我们对一个未知的状态的误判：
> 
> <img src="https://gitlab.com/mtdickens1998/mtd-images/-/raw/main/pictures/2025/01/22_18_23_31_20250122182331.png" width="50%"/>
> 
> 如上图
> 
> - 红色的线就是 $\pi_0$
> - 黄色的线就是 $\pi_f$
> 
> 对于一个只有少量几个参数的情况，这种 Ver 0.5 的简单粗暴的方法是可行的（比如机器人学中的所谓“系统识别 (system identification)”）
> 
> 但是参数一多（比如神经网络），就容易导致上图中的误判

## Can We Do Better?

我们希望，$p_{\pi_0}(s_t) \approx p_{\pi_f}(s_t)$

为了这样，我们就需要反复往 buffer $\mathcal D$ 中加入使用当前策略采集得到的数据，并且丢弃老数据。随着 $f$ 收敛，$\mathcal D$ 中的分布会越来越接近 $\pi_f$ 实际的分布。

Model-Based RL Version 1.0

1. run base policy $\pi_0\left(\mathbf{a}_t \mid \mathbf{s}_t\right)$ (e.g., random policy) to collect $\mathcal{D}=\left\{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)_i\right\}$
2. learn dynamics model $f(\mathbf{s}, \mathbf{a})$ to minimize $\sum_i\left\|f\left(\mathbf{s}_i, \mathbf{a}_i\right)-\mathbf{s}_i^{\prime}\right\|^2$
3. plan through $f(\mathbf{s}, \mathbf{a})$ to choose actions
4. execute those actions and add the resulting data $\left\{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)_j\right\}$ to $\mathcal D$, goto (2)

> [!question] Problem
> 
> 由于我们是 plan through $f(s, a)$ to choose actions，因此如果模型有问题，那么有时候就会一错再错。
> 
> 比如说：我们的模型错误地认为，【如果方向盘左偏 2 度，就可以往正前方开】。那么，我们如果真的这样进行了 plan，那么就相当于是【我们一直保持方向盘往左偏 2 度的 action】。这样就会导致我们整个轨迹犯大错，但是直到我们结束了整个轨迹之后，才会进行修正，从而效率很低。
> 
> 如果我们每走一步，就根据当前状态进行 replan，那么就可以立刻进行修正，效率高很多。

因此，Model-Based RL Vers. 1.5 就是：

1. run base policy $\pi_0\left(\mathbf{a}_t \mid \mathbf{s}_t\right)$ (e.g., random policy) to collect $\mathcal{D}=\left\{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)_i\right\}$
2. learn dynamics model $f(\mathbf{s}, \mathbf{a})$ to minimize $\sum_i\left\|f\left(\mathbf{s}_i, \mathbf{a}_i\right)-\mathbf{s}_i^{\prime}\right\|^2$
3. plan through $f(\mathbf{s}, \mathbf{a})$ to choose actions
4. execute the **first** planned action, observe resulting state $s'$ (MPC)
    - MPC: Model Predictive Control，文中的意思就是【进行闭环控制】——每走一步，就重新进行 planning
5. append $(s, a, s')$ to dataset $\mathcal D$
6. goto (2) every N steps, else goto (3)

### How to Replan

上面这个方法，要求我们每走一步就进行 replan。表面上看起来，这样做会非常 computationally expensive。但是，实际上，the more you replan, the less perfect each individual plan needs to be。

因此，我们可以这样 make planning less perfect *but more computationally efficient*"

- 使用 shorter horizons（也就是 plan 的时候，只看后面短短几步）
- 甚至可以直接用 random sampling

## Uncertainty in MBRL

<img src="https://gitlab.com/mtdickens1998/mtd-images/-/raw/main/pictures/2025/01/23_20_37_3_20250123203703.png"/>
注：

- Mb: Model-based
- Mf: Model-free
- Mb-Mf: 先 Mb，再 Mf。论文中是先用 Mb 训练出来一个 dynamics model，通过 MPC 收集大量的 trajectories 到 $\mathcal D^\ast$，然后 initialize 一个随机策略 $\pi_\theta(a|s)$，对着 $\mathcal D^\ast$ 进行模仿学习。学完之后，再对这个 $\pi_\theta$ 来进行 Mf 训练

在 [A Nagabandi et al. 17](https://arxiv.org/abs/1708.02596)，如上图，如果使用 Mb-Mf 的方法，就可以相比纯粹 Mf 快好几倍。这是显然的，毕竟可以用 Mb 很快打好基础，然后在基础上进行训练，就会很快。

但是，图中没有列出的是，如果使用纯 Mb，那么效果会非常差。这还是因为过拟合：

<img src="https://gitlab.com/mtdickens1998/mtd-images/-/raw/main/pictures/2025/01/23_20_46_7_20250123204606.png"/>

如上图，图中出现了过拟合的情况，但是不算非常严重，因此只有很小一段 $(-5, -4)$ 产生了过拟合：

- 对于以往的不管是 policy gradient 还是 Q-learning，即便有过拟合，由于只有很少的比例过拟合，因此并无大碍
- 但是，使用 model-based RL，我们的 planning 会
	1. 主动寻找这些峰值高的部分
	2. 因此，即便峰值高的部分在所有情况中中占比少
	3. 但是由于所有情况很多
	4. 因此峰值高的本身数量不少
	5. 算法主动去寻找这些部分，从而导致不论在什么状态，算法很容易找到这些部”虚高的状态“
	6. 找到了之后，算法就会倾向于规划一条经过这个状态的 plan
- 假设这些虚高的尖峰有 1000 万个，那么我就可能需要跑一千万次，才能将这些尖峰都补上

因此，我们就希望能够在对 dynamics, rewards 建模的同时，还可以对 uncertainty 建模。

> [!question]+ 如何对 uncertainty 建模？
> 
> 如何对 uncertainty 建模呢？粗略来说，给定一个 variance（图中我们用圆圈进行可视化表示），我们可以求出三种 value。
> 
> 为了便于说明，不妨假设我们这里的 $r(s_t, a_t)$ 只和 $s_{t+1}$ 有关，那么，此处的 value 就是 value of next state:
> 
> <img src="https://gitlab.com/mtdickens1998/mtd-images/-/raw/main/pictures/2025/01/24_10_19_4_20250124101903.png" width="50%"/>
> 
> - expected value (of next state)
> 	- 对于图中小圈，大概站稳
> 	- 对于图中大圈，必然掉进海里
> 	- 一个很不错的训练方法
> - pessimistic value，比如说就是 95% 置信区域中最差的
> 	- 对于图中小圈，大概掉进海里
> 	- 对于图中大圈，必然掉进海里
> 	- 如果需要 safety，那么可以用
> - optimistic value，比如说就是 70% 置信区域中最好的
> 	- 对于图中小圈，必然站稳
> 	- 对于图中大圈，大概掉进海里
> 	- 如果需要更多 exploitation，那么可以用
> 
> **注意**：假设我们这里的 $r(s_t, a_t)$ 只和 $s_{t+1}$ 有关。那么，
> 
> 1. 图中的五角星，就是 expected next state (i.e. $E[s_{t+1}]$)
> 2. 五角星所在的地方的 value，就是 value expected next state (i.e. $V(E[s_{t+1}]))$)
> 3. 但是，这并不等于 expected value of next state (i.e. $E[V(s_{t+1})]$)
> 
> 实际建模中，我们可以对 $p_\theta(s, a)$ 和 $r_\mu(s, a)$ 都进行这样的 uncertainty 建模。
> 
> 另外，下文中，我们用 expected value。

那么，我们具体用什么方法对 uncertainty 进行建模？
### Idea 1: use output entropy

> [!warning] tl; dr: 警告
> 
> 这是非常错误的方法，请勿使用

我们能否加上一个所谓的”熵正则化“？就是在最后的 reward 中，减去 $\alpha H[X_\text{output}]$（注: $H[X]$ 就是 $X$ 的熵）。

答案是不可以的。原因如下：

---

我们有两种 uncertainty——两者的概念完全正交：

1. Aleatoric/Statistical Uncertainty: Dynamics 本身的 uncertainty
2. Epistemic/Model Uncertainty: （由于数据不够），我们对 model 本身参数的 uncertainty

**前者就是**：

Dynamics 本身就是随机的。比如 ”掷骰子“ 这个动作，本身就会导致【1~6 点数各 $\frac 1 6$ 的概率】。这是环境自身所决定的，你训练的再多，也不能让 dynamics model 收敛到【某个点数概率为 1】。

**后者就是**：

<img src="https://gitlab.com/mtdickens1998/mtd-images/-/raw/main/pictures/2025/01/24_16_59_38_20250124165938.png" width="50%"/>

我们本身没有收集足够多的数据，因此我们对 dynamics model 本身的参数就存在困惑。

### Idea 2: estimate model uncertainty

我们需要 estimate 每一个 model 的 uncertainty。简单粗暴的话，直接用 bootstrap ensembles (bagging) 就行；复杂一些的话，可以用 Bayesian Neural Network。

### Bootstrap Ensembles (Bagging)

如何估计 $p(\theta|\mathcal D)$？我们可以构建 $N$ 个不同的神经网络，：

$$
\begin{aligned}
&\text{然后做出下面的假设：}  & &p(\theta|\mathcal D) \approx \frac 1 N \sum_i \delta(\theta_i) &\newline
&\text{从而就有：} & &\int p(s_{t+1}|s_t, a_t, \theta) p(\theta|\mathcal D) \mathrm d\theta \approx \frac 1 N \sum_i p(s_{t+1}|s_t, a_t, \theta)&
\end{aligned}
$$

其中，不同的 $\theta_i$，理论上应该使用相互独立的数据集 $\mathcal D_i$。从理论上来说，可以用 sample with replacement 来从 $\mathcal D$ 中抽取；实际中，SGD 本身带来的随机性已经够强了，因此都用 $\mathcal D$ 就行了。

*(TODO)*